== SOURCE
https://www.theverge.com/2020/4/30/21243038/openai-jukebox-model-raw-audio-lyrics-ai-generated-copyright
== AGENT
OpenAI
== GOAL
Jukebox is a new generative model that creates music. The aim was to generate raw audio with that was recognisable music, with lyrics, chords, audio and other features. Jukebox built on the previous OpenAI project MuseNet which used MIDI data to generate music. However, symbolic generators lie MuseNet cannot capture human voices and other subtle effects. Using raw audio allows Jukebox to tackle sounds similar to human voices but the challenge lies in the fact that a standard song has over ten million timesteps. For comparison, OpenAI's GPT-2 had only 1000 timesteps.
== DATA
The dataset used consisted of 1.2 million songs, half of which were in English. This audio was then paired with lyrics and other metadata from LyricWiki that included various genre keywords, the artist, the album, moods and release years. The audio was then downmixed on both left and right channels to produce mono channel audio. 
== METHODS
Jukebox compresses raw audio using an unsupervised quantization-based approach called VQ-VAE. The 44kHz raw audio is compressed by 8x, 32x and 128x. Much of the detail in the audio is lost in this process and would be unrecognisable as music but retains the essential information. Once compressed the prior models learn the distribution of the codes generated by the VQ-VAE. The top-level prior captures elements like sing and melodies while the middle and bottom priors begin to put the audio back together improving the audio quality. Once the priors are trained they are unsampled and decoded into raw audio. The top level is also trained to with the genre and artist metadata so that the generation can be steered towards a certain style. 
== RESULTS
Among the results generated by Jukebox the research team was able to use it to cluster artists and genres together with t-SNE. Jukebox yields more impressive results than its predecessors but sometimes the audio quality can be poor from the sampling process and it can't replicate typical music structures such as repeated verses and choruses. However, it does generate pieces that are coherent, follow chord patterns and sometimes creates impressive solos. On top of the noise introduced in the sampling and unsampling process it means it can take about 9 hours to generate one minute of music. Some of the more impressive results are some of the generated pieces that perform songs with alternative artists like Kanye West convincingly performing Eminem's Lose Yourself.
== ISSUES
The research team were honest about the some of the best examples being cherry picked and provided the library of unsaturated examples that show how noise and unintelligible some of the results can be. Another more ethical issue highlighted in the article is the copyright implications of using music for training and generating new music using artist's voices.
== SCORE
6
== COMMENTS
While this work is very interesting and surprising, especially when it works well it is very experimental which is again highlighted by the researchers themselves. Jukebox was built to improve upon MuseNet which it certainly has done. The researchers raise the idea that future work in this area could bring about new tools for human artists to work with in the production of music or even for human musicians to collaborate with models which would be an interesting experiment. The article itself is light on details regarding the training of the models and does not offer many good examples of the generation.
