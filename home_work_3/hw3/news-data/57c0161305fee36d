== SOURCE
https://www.theverge.com/2019/10/30/20939147/deepmind-google-alphastar-starcraft-2-research-grandmaster-level
== AGENT
DeepMind
== GOAL
AlphaStar is an AI developed by Deepmind following the success of their AI AlphaGo in an attempt to replicate that success in the real time strategy game StarCraft2 developed by Blizzard Entertainment. The aim is that the AI will be able to beat the best professional human players in competition without "cheating" in the way that many video game AIs do. The difference between Go and other board games and StarCraft is that in StarCraft the players have imperfect information, parts of the map in StarCraft are invisible to the player whereas in Go or Chess each player can has perfect information on the board state. StarCraft also has a state-space complexity of 10^1685 compared to Go's 10^170 on a 19x19 board or Chess's 10^47.
== DATA
DeepMind partnered with Blizzard Entertainment to make real game data available. The AI was originally trained on this set of at least half a million game replays released by Blizzard. The data takes the shape of a piece of code for every action taken at each time interval throughout the game that can then be replicated by calling the actions through Blizzard's API. For example the rectangle select action used by a player to select units in a rectangle between two points with left click on one point and a release on another point translates to select_rect(p1,p2).
== METHODS
AlphaStar's neural network was originally trained on the data from real games released by Blizzard which falls under supervised learning. This allowed AlphaStar to imitate the basic strategies used by real players. Following this training the agent was able to defeat the games built in Elite AI in 95% of games. Using this baseline AlphaStar began to be trained with Reinforcement learning by placing agents within a league of other agents allowing them to play against each other and develop different variations. This allowed the AI to be exposed a wide range of strategies similarly to how real players climb the rankings. Each agent was given a learning objective to encourage diversity strategies in the league with some agents developing strategies to specifically counter those of other agents. With each agent powered by 16 of Google's v3 Cloud TPUs each agent engaged in 200 years of real time games over the course of 14 days.
== RESULTS
After training AlphaStar was pitted against professional players. The first of which was known as TLO, AlphaStar made some obvious and strange mistakes but won five games out of five with unique and interesting strategies in each match. After another week of reinforcement training AlphaStar played against another pro known as MaNa, this time one of the top 10 players in the type of match they would be playing. Again AlphaStar won all five games with the professional player commenting on how his style of forcing his opponents to make mistakes simply doesn’t work against AlphaStar. In these ten games even though AlphaStar's actions and reaction speed were restricted to be at the same level as the top professionals it still have the distinct advantage of not being restricted by the in game field of view in the way that a real player is. After implementation of these restrictions the new agent lost the rematch with MaNa as it struggled to manipulate the camera.
== ISSUES
One of the biggest issues with the reinforcement learning used to refine AlphaStar is that it cannot compare to human's ability to analyse a small number of games and learn from them. After his initial five games MaNa admitted to tailoring his response for the rematch after analysing his five losses. Five games is simply not enough data for AlphaStar to learn from in a meaningful way. Unlike DeepMinds AlphaGo, AlphaStar cannot be adapted in the same way to other applications like AlphaGo was to chess. When AlphaGo was taught how to play chess it mastered it in eight hours. AlphaStar doesn’t work with the same easy to replace rule set as AlphaGo does. 
== SCORE
9
== COMMENTS
AlphaStar is a very interesting application in several ways. For one it is a huge step up from AlphaGo in terms of the complexity of the game it’s playing as well as the fact that it's playing with incomplete information. The data it initially used is new and open source for anyone to experiment with. Perhaps its most interesting trait is the reinforcement learning where each agent played against slightly different versions of itself with the goal of finding the strategy to break the other agent. This type of learning allowed the agents to learn new strategies while never forgetting old ones. The progress that a short amount of time in the real world can allow for is also impressive. In between the two pros that AlphaStar played against it had managed to iron out most of the bizarre and questionable mistakes in its strategies. The author of the article had a good understanding of the surface level functionality and was able to highlight the applications that DeepMind is hoping to tackle in the future with what it has learned from AlphaStar.
